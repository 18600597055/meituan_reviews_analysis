{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import  matplotlib.pyplot as  plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>还行吧，建议不要排队那个烤鸭和羊肉串，因为烤肉时间本来就不够，排那个要半小时，然后再回来吃烤...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>去过好几次了 东西还是老样子 没增添什么新花样 环境倒是挺不错 离我们这也挺近 味道还可以 ...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>一个字：好！！！ #羊肉串# #五花肉# #牛舌# #很好吃# #鸡软骨# #拌菜# #抄河...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>第一次来吃，之前看过好多推荐说这个好吃，真的抱了好大希望，排队的人挺多的，想吃得趁早来啊。还...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>羊肉串真的不太好吃，那种说膻不膻说臭不臭的味。烤鸭还行，大虾没少吃，也就到那吃大虾了，吃完了...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  star\n",
       "0  还行吧，建议不要排队那个烤鸭和羊肉串，因为烤肉时间本来就不够，排那个要半小时，然后再回来吃烤...    40\n",
       "1  去过好几次了 东西还是老样子 没增添什么新花样 环境倒是挺不错 离我们这也挺近 味道还可以 ...    40\n",
       "2  一个字：好！！！ #羊肉串# #五花肉# #牛舌# #很好吃# #鸡软骨# #拌菜# #抄河...    50\n",
       "3  第一次来吃，之前看过好多推荐说这个好吃，真的抱了好大希望，排队的人挺多的，想吃得趁早来啊。还...    20\n",
       "4  羊肉串真的不太好吃，那种说膻不膻说臭不臭的味。烤鸭还行，大虾没少吃，也就到那吃大虾了，吃完了...    30"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_excel(\"all_data_meituan.xlsx\")[[\"comment\",\"star\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17400, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment']=df['star'].apply(lambda x:1 if x>30 else 0)\n",
    "df=df.drop_duplicates() ## 去掉重复的评论\n",
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3138, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=pd.concat([df[['comment']],df[['comment']],df[['comment']]])\n",
    "y=pd.concat([df.sentiment,df.sentiment,df.sentiment])\n",
    "X.columns=['comment']\n",
    "X.reset_index\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\HUANG_~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.031 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    还行 吧 ， 建议 不要 排队 那个 烤鸭 和 羊肉串 ， 因为 烤肉 时间 本来 就 不够...\n",
       "1    去过 好 几次 了   东西 还是 老 样子   没 增添 什么 新花样   环境 倒 是 ...\n",
       "2    一个 字 ： 好 ！ ！ ！   # 羊肉串 #   # 五花肉 #   # 牛舌 #   ...\n",
       "3    第一次 来 吃 ， 之前 看过 好多 推荐 说 这个 好吃 ， 真的 抱 了 好 大 希望 ...\n",
       "4    羊肉串 真的 不太 好吃 ， 那种 说 膻 不 膻 说 臭 不 臭 的 味 。 烤鸭 还 行...\n",
       "Name: cut_comment, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "def chinese_word_cut(mytext):\n",
    "    return \" \".join(jieba.cut(mytext))\n",
    "X['cut_comment']=X[\"comment\"].apply(chinese_word_cut)\n",
    "X['cut_comment'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import  train_test_split\n",
    "X_train,X_test,y_train,y_test= train_test_split(X,y,random_state=42,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_custom_stopwords(stop_words_file):\n",
    "    with open(stop_words_file,encoding=\"utf-8\") as f:\n",
    "        custom_stopwords_list=[i.strip() for i in f.readlines()]\n",
    "    return custom_stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100', '01', '02', '03', '04', '05', '06', '07', '08', '09']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_file = \"stopwords.txt\"\n",
    "stopwords = get_custom_stopwords(stop_words_file)\n",
    "stopwords[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import  CountVectorizer\n",
    "vect=CountVectorizer()\n",
    "vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2353x1965 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 20491 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.fit_transform(X_train[\"cut_comment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2353, 1965)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.fit_transform(X_train[\"cut_comment\"]).toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(vect.fit_transform(X_train[\"cut_comment\"]).toarray(),columns=vect.get_feature_names()).iloc[:10,:22]\n",
    "# print(vect.get_feature_names())\n",
    "# #  数据维数1956，不算很大（未使用停用词）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amazing</th>\n",
       "      <th>happy</th>\n",
       "      <th>ktv</th>\n",
       "      <th>pm2</th>\n",
       "      <th>一万个</th>\n",
       "      <th>一个多</th>\n",
       "      <th>一个月</th>\n",
       "      <th>一串</th>\n",
       "      <th>一人</th>\n",
       "      <th>一件</th>\n",
       "      <th>...</th>\n",
       "      <th>麻烦</th>\n",
       "      <th>麻酱</th>\n",
       "      <th>黄喉</th>\n",
       "      <th>黄桃</th>\n",
       "      <th>黄花鱼</th>\n",
       "      <th>黄金</th>\n",
       "      <th>黑乎乎</th>\n",
       "      <th>黑椒</th>\n",
       "      <th>黑胡椒</th>\n",
       "      <th>齐全</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1691 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   amazing  happy  ktv  pm2  一万个  一个多  一个月  一串  一人  一件 ...  麻烦  麻酱  黄喉  黄桃  \\\n",
       "0        0      0    0    0    0    0    0   0   0   0 ...   0   0   0   0   \n",
       "1        0      0    0    0    0    0    0   0   0   0 ...   0   0   0   0   \n",
       "2        0      0    0    0    0    0    0   0   0   0 ...   0   0   0   0   \n",
       "3        0      0    0    0    0    0    0   0   0   0 ...   0   0   0   0   \n",
       "4        0      0    0    0    0    0    0   0   0   0 ...   0   0   0   0   \n",
       "\n",
       "   黄花鱼  黄金  黑乎乎  黑椒  黑胡椒  齐全  \n",
       "0    0   0    0   0    0   0  \n",
       "1    0   0    0   0    0   0  \n",
       "2    0   0    0   0    0   0  \n",
       "3    0   0    0   0    0   0  \n",
       "4    0   0    0   0    0   0  \n",
       "\n",
       "[5 rows x 1691 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(token_pattern=u'(?u)\\\\b[^\\\\d\\\\W]\\\\w+\\\\b',stop_words=frozenset(stopwords)) # 去除停用词\n",
    "pd.DataFrame(vect.fit_transform(X_train['cut_comment']).toarray(), columns=vect.get_feature_names()).head()\n",
    "# 1691 columns,去掉以数字为特征值的列，减少了三列编程1691 \n",
    "# max_df = 0.8 # 在超过这一比例的文档中出现的关键词（过于平凡），去除掉。\n",
    "# min_df = 3 # 在低于这一数量的文档中出现的关键词（过于独特），去除掉。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import  metrics\n",
    "svc_cl=SVC()\n",
    "pipe=make_pipeline(vect,svc_cl)\n",
    "pipe.fit(X_train.cut_comment, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63184713375796175"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = pipe.predict(X_test.cut_comment)\n",
    "metrics.accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0, 289],\n",
       "       [  0, 496]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  支持向量机分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63184713375796175"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc_cl=SVC() # 实例化\n",
    "pipe=make_pipeline(vect,svc_cl)\n",
    "pipe.fit(X_train.cut_comment, y_train)\n",
    "y_pred = pipe.predict(X_test.cut_comment)\n",
    "metrics.accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 支持向量机 网格搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import  Pipeline\n",
    "# svc=SVC(random_state=1)\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf=TfidfTransformer()\n",
    "# ('tfidf',\n",
    "#                       TfidfTransformer()),\n",
    "#                      ('clf',\n",
    "#                       SGDClassifier(max_iter=1000)),\n",
    "# svc=SGDClassifier(max_iter=1000)\n",
    "svc=SVC()\n",
    "# pipe=make_pipeline(vect,SVC)\n",
    "pipe_svc=Pipeline([(\"scl\",vect),('tfidf',tfidf),(\"clf\",svc)])\n",
    "para_range=[0.0001,0.001,0.01,0.1,1.0,10,100,1000]\n",
    "para_grid=[\n",
    "    {'clf__C':para_range,\n",
    "    'clf__kernel':['linear']},\n",
    "    {'clf__gamma':para_range,\n",
    "    'clf__kernel':['rbf']}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs=GridSearchCV(estimator=pipe_svc,param_grid=para_grid,cv=10,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('scl', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=frozenset({'...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid=[{'clf__C': [0.0001, 0.001, 0.01, 0.1, 1.0, 10, 100, 1000], 'clf__kernel': ['linear']}, {'clf__gamma': [0.0001, 0.001, 0.01, 0.1, 1.0, 10, 100, 1000], 'clf__kernel': ['rbf']}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train.cut_comment,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scl', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=frozenset({'...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_.fit(X_train.cut_comment,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95031847133757963"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = gs.best_estimator_.predict(X_test.cut_comment)\n",
    "metrics.accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 临近法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=...owski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=4, p=2,\n",
       "           weights='uniform'))])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import  KNeighborsClassifier\n",
    "knn=KNeighborsClassifier(n_neighbors=4,p=2,metric='minkowski')\n",
    "pipe=make_pipeline(vect,knn)\n",
    "pipe.fit(X_train.cut_comment, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81910828025477711"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = pipe.predict(X_test.cut_comment)\n",
    "metrics.accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[188, 101],\n",
       "       [ 41, 455]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree=DecisionTreeClassifier(criterion='entropy',random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93885350318471339"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe=make_pipeline(vect,tree)\n",
    "pipe.fit(X_train.cut_comment, y_train)\n",
    "y_pred = pipe.predict(X_test.cut_comment)\n",
    "metrics.accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[256,  33],\n",
       "       [ 15, 481]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96560509554140128"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest=RandomForestClassifier(criterion='entropy',random_state=1,n_jobs=2)\n",
    "pipe=make_pipeline(vect,forest)\n",
    "pipe.fit(X_train.cut_comment, y_train)\n",
    "y_pred = pipe.predict(X_test.cut_comment)\n",
    "metrics.accuracy_score(y_test,y_pred)\n",
    "# 加上tfidf反而准确率96.5至95.0，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[265,  24],\n",
       "       [  3, 493]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bagging方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95541401273885351"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "bag=BaggingClassifier(base_estimator=tree,\n",
    "                     n_estimators=10,\n",
    "                     max_samples=1.0,\n",
    "                     max_features=1.0,\n",
    "                     bootstrap=True,\n",
    "                     bootstrap_features=False,\n",
    "                     n_jobs=1,random_state=1)\n",
    "pipe=make_pipeline(vect,tfidf,bag)\n",
    "pipe.fit(X_train.cut_comment, y_train)\n",
    "y_pred = pipe.predict(X_test.cut_comment)\n",
    "metrics.accuracy_score(y_test,y_pred)  #  没用转化td-idf 93.2%, 加上转化步骤，准确率提升到95.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[260,  29],\n",
       "       [  6, 490]], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Gradient Boosting方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.18, loss='deviance', max_depth=10,\n",
      "              max_features='sqrt', max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.01, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=240,\n",
      "              presort='auto', random_state=42, subsample=0.9, verbose=0,\n",
      "              warm_start=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.96560509554140128"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "grd = GradientBoostingClassifier(learning_rate=0.18,max_depth=10,n_estimators=240,random_state=42,max_features='sqrt',subsample=0.9,\n",
    "                                min_impurity_decrease=0.01)\n",
    "                                \n",
    "print(grd)\n",
    "# Choosing subsample < 1.0 leads to a reduction of variance and an increase in bias.\n",
    "# Choosing max_features < n_features leads to a reduction of variance and an increase in bias.降低过拟合，但是可能会增加偏差，降低方差（对应的过拟合）\n",
    "pipe=make_pipeline(vect,tfidf,grd)\n",
    "pipe.fit(X_train.cut_comment, y_train)\n",
    "y_pred = pipe.predict(X_test.cut_comment)\n",
    "metrics.accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[265,  24],\n",
       "       [  3, 493]], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### xgboost 方法 sklearn类似 api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() got an unexpected keyword argument 'num_boost_round'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-c640fb2355d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m )\n\u001b[0;32m     31\u001b[0m \u001b[0mevals\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_vect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_vect\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0meval_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: fit() got an unexpected keyword argument 'num_boost_round'"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier   \n",
    "# sklearn API 类似于导入的从skearn中导入某个算法，然后再进行实例化即可，初始化算法的时候可以修改默认参数\n",
    "from xgboost import plot_importance\n",
    "x_train_vect=vect.fit_transform(X_train[\"cut_comment\"])\n",
    "x_test_vect= vect.transform(X_test[\"cut_comment\"])\n",
    "clf = XGBClassifier(\n",
    "silent=1 ,#设置成1则没有运行信息输出，最好是设置为0.是否在运行升级时打印消息。\n",
    "# #nthread=4,# cpu 线程数 默认最大\n",
    "learning_rate= 0.20, # 学习率\n",
    "min_child_weight=0.5, \n",
    "# # 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言\n",
    "# #，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。\n",
    "# #这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。\n",
    "gamma=0.1,  # 树的叶子节点上作进一步分区所需的最小损失减少,越大越保守，一般0.1、0.2这样子。\n",
    "subsample=0.7, # 随机采样训练样本 训练实例的子采样比\n",
    "max_depth=15,\n",
    "max_delta_step=0,#最大增量步长，我们允许每个树的权重估计。\n",
    "colsample_bylevel=0.7, # Subsample ratio of columns for each split, in each level.\n",
    "colsample_bytree=0.6, # 生成树时进行的列采样 \n",
    "reg_lambda=0.0001,  # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越保守\n",
    "reg_alpha=0.003, # L1 正则项参数，参数越大，模型越保守\n",
    "### 正则化是在梯度提升树种没有的，这是xgboost与GB方法的区别之一。\n",
    "scale_pos_weight=1, #如果取值大于0的话，在类别样本不平衡的情况下有助于快速收敛。平衡正负权重=sum(负类样本)/sum(正类样本)\n",
    "# objective= 'reg:logistic', #多分类的问题 指定学习任务和相应的学习目标\n",
    "objective='binary:logistic' ,\n",
    "# #num_class=10, # 类别数，多分类与 multisoftmax 并用\n",
    "n_estimators=800, #树的个数\n",
    "random_state=42\n",
    "# #eval_metric= 'auc'\n",
    ")\n",
    "evals  = [(x_test_vect, y_test)]\n",
    "clf.fit(x_train_vect,y_train,eval_set=evals,early_stopping_rounds=150,num_boost_round=1500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94777070063694269"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=clf.predict(x_test_vect)\n",
    "metrics.accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[257,  32],\n",
       "       [  9, 487]], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xgboost learning  API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vect=vect.fit_transform(X_train[\"cut_comment\"])\n",
    "x_test_vect= vect.transform(X_test[\"cut_comment\"])\n",
    "import xgboost as xgb\n",
    "dtrain = xgb.DMatrix(x_train_vect, label=y_train)\n",
    "dtest=xgb.DMatrix(x_test_vect,label=y_test)\n",
    "evals  = [(dtest, 'eval')]\n",
    "param = {'max_depth':15, 'eta':0.2, 'silent':1, 'objective':'binary:logistic','n_estimators':800, 'gamma':0.1,'colsample_bytree':0.6,\n",
    "        'alpha':0.003,'lambda':0.04, 'colsample_bylevel':0.7,'subsample':0.7,'min_child_weight':0.5,'evals':evals,'early_stopping_rounds':150 }\n",
    "\n",
    "bst = xgb.train(param, dtrain,1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n"
     ]
    }
   ],
   "source": [
    "print(bst.best_ntree_limit)\n",
    "y_pred = bst.predict(dtest,ntree_limit=bst.best_ntree_limit)\n",
    "y_pred=[1 if i>0.5 else 0 for i in y_pred]\n",
    "# predictions = [round(value) for value in y_pred]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95541401273885351"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[257,  32],\n",
       "       [  3, 493]], dtype=int64)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test,y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "194px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
